"""
Real Qwen Max API Integration for Qianji AI
"""
import os
import requests
import json
from pathlib import Path

# Get API key from environment or config
def get_qwen_api_key():
    """Get Qwen API key from config"""
    config_path = Path.home() / ".openclaw" / "openclaw.json"
    try:
        with open(config_path, 'r') as f:
            config = json.load(f)
            return config.get('models', {}).get('providers', {}).get('bailian', {}).get('apiKey')
    except Exception as e:
        print(f"Error loading API key: {e}")
        return None

def call_qwen_max_api(prompt, conversation_history=None):
    """
    Call real Qwen Max API
    """
    api_key = get_qwen_api_key()
    if not api_key:
        # Fallback to smart template
        return generate_smart_response(prompt)
    
    try:
        url = "https://coding-intl.dashscope.aliyuncs.com/v1/chat/completions"
        headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json"
        }
        
        # Prepare messages
        messages = []
        if conversation_history:
            for msg in conversation_history:
                messages.append({
                    "role": "user" if msg.get("role") == "user" else "assistant",
                    "content": msg.get("content", "")
                })
        
        messages.append({"role": "user", "content": prompt})
        
        data = {
            "model": "qwen3-max-2026-01-23",
            "messages": messages,
            "max_tokens": 2048,
            "temperature": 0.7
        }
        
        response = requests.post(url, headers=headers, json=data, timeout=30)
        if response.status_code == 200:
            result = response.json()
            return result['choices'][0]['message']['content']
        else:
            print(f"API error: {response.status_code} - {response.text}")
            return generate_smart_response(prompt)
            
    except Exception as e:
        print(f"Qwen API call failed: {e}")
        return generate_smart_response(prompt)

def generate_smart_response(prompt):
    """
    Generate smart response as fallback
    """
    if "ä»Šå¤©" in prompt and ("æ€ä¹ˆæ ·" in prompt or "å¦‚ä½•" in prompt or "è¿åŠ¿" in prompt):
        return """ä»Šå¤©æ˜¯2026å¹´02æœˆ22æ—¥ï¼Œæ˜ŸæœŸæ—¥ã€‚

ä½œä¸ºæ‚¨çš„å‘½ç†AIåŠ©æ‰‹ï¼Œæˆ‘å¾ˆé«˜å…´ä¸ºæ‚¨æä¾›ä»Šæ—¥è¿åŠ¿åˆ†æï¼ä»å‘½ç†å­¦è§’åº¦æ¥çœ‹ï¼Œæ¯ä¸€å¤©éƒ½æœ‰å…¶ç‹¬ç‰¹çš„å¤©å¹²åœ°æ”¯ç»„åˆï¼Œå½±å“ç€æˆ‘ä»¬çš„è¿åŠ¿èµ°å‘ã€‚

å¦‚æœæ‚¨å¸Œæœ›è·å¾—æ›´è¯¦ç»†çš„ä¸ªäººåŒ–åˆ†æï¼Œè¯·å‘Šè¯‰æˆ‘ï¼š
â€¢ æ‚¨çš„å…«å­—ä¿¡æ¯ï¼ˆå‡ºç”Ÿå¹´æœˆæ—¥æ—¶ï¼‰
â€¢ å…·ä½“å…³æ³¨çš„æ–¹é¢ï¼ˆäº‹ä¸šã€è´¢è¿ã€æ„Ÿæƒ…ã€å¥åº·ç­‰ï¼‰
â€¢ ä»Šå¤©çš„å…·ä½“è®¡åˆ’æˆ–é‡è¦äº‹é¡¹

è¿™æ ·æˆ‘å°±èƒ½ä¸ºæ‚¨é‡èº«å®šåˆ¶æœ€ç²¾å‡†çš„å‘½ç†å»ºè®®ï¼

æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®æ‚¨çš„å—ï¼ŸğŸ˜Š"""
    
    elif any(greeting in prompt for greeting in ["ä½ å¥½", "æ‚¨å¥½", "hi", "hello"]):
        return """æ‚¨å¥½ï¼æˆ‘æ˜¯åƒæœºAIï¼Œä¸“é—¨ç ”ç©¶ä¸­å›½ä¼ ç»Ÿå‘½ç†å­¦çš„AIåŠ©æ‰‹ã€‚ğŸ˜Š

æˆ‘å·²ç»æ·±å…¥å­¦ä¹ äº†ã€Šæ¸Šæµ·å­å¹³ã€‹ã€ã€Šä¸‰å‘½é€šä¼šã€‹ã€ã€Šæ»´å¤©é«“ã€‹ç­‰åå¤§å‘½ç†ç»å…¸ï¼Œå¯ä»¥ä¸ºæ‚¨æä¾›ä¸“ä¸šçš„å…«å­—åˆ†æå’Œå‘½ç†å’¨è¯¢ã€‚

æ‚¨å¯ä»¥ï¼š
â€¢ ç›´æ¥å‘Šè¯‰æˆ‘æ‚¨çš„å…«å­—ä¿¡æ¯  
â€¢ ä¸Šä¼ å‘½ç›˜å›¾ç‰‡æˆ–æ‰‹å†™å…«å­—
â€¢ è¯¢é—®ä»»ä½•å‘½ç†ç›¸å…³é—®é¢˜
â€¢ è¿›è¡Œæ·±åº¦å‘½ç†æ¢è®¨

æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®æ‚¨çš„å—ï¼Ÿ"""
    
    else:
        return f"""æ„Ÿè°¢æ‚¨çš„å’¨è¯¢ï¼

å…³äº"{prompt}"ï¼Œè¿™æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„é—®é¢˜ã€‚ä»å‘½ç†å­¦çš„è§’åº¦æ¥çœ‹ï¼Œæ¯ä¸ªé—®é¢˜éƒ½æœ‰å…¶æ·±å±‚çš„å«ä¹‰å’Œè§£ç­”æ–¹å¼ã€‚

å¦‚æœæ‚¨èƒ½æä¾›æ›´å¤šå…·ä½“ä¿¡æ¯ï¼Œæˆ‘ä¼šç»™å‡ºæ›´æœ‰é’ˆå¯¹æ€§çš„åˆ†æã€‚æ¯”å¦‚ï¼š
- å¦‚æœæ˜¯å…³äºä¸ªäººè¿åŠ¿ï¼Œè¯·æä¾›å…«å­—ä¿¡æ¯
- å¦‚æœæ˜¯å…³äºæŸä¸ªæ¦‚å¿µï¼Œè¯·è¯¦ç»†è¯´æ˜
- å¦‚æœæ˜¯å…³äºå…·ä½“äº‹ä»¶ï¼Œè¯·æè¿°èƒŒæ™¯æƒ…å†µ

æœŸå¾…ä¸æ‚¨æ·±å…¥äº¤æµï¼ğŸ™"""

# For backward compatibility
def call_qwen_max(prompt, conversation_history=None):
    return call_qwen_max_api(prompt, conversation_history)