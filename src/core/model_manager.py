"""
Model Manager for Qji Max - Integration of Qwen Max with fine-tuned weights
"""
import os
import torch
from pathlib import Path
from transformers import AutoTokenizer, AutoModelForCausalLM

class ModelManager:
    def __init__(self):
        self.model = None
        self.tokenizer = None
        self.model_path = Path("/Users/kirin/Projects/qianji/models/qji_max")
        self.base_model_name = "bailian/qwen3-max-2026-01-23"
        
    def load_model(self):
        """Load Qji Max model (Qwen Max base + fine-tuned weights)"""
        print("ğŸ”„ æ­£åœ¨åŠ è½½Qji Maxæ¨¡å‹...")
        
        try:
            # Check if fine-tuned model exists
            if self.model_path.exists():
                print(f"âœ… æ‰¾åˆ°Qji Maxå¾®è°ƒæ¨¡å‹: {self.model_path}")
                self.tokenizer = AutoTokenizer.from_pretrained(str(self.model_path))
                self.model = AutoModelForCausalLM.from_pretrained(
                    str(self.model_path),
                    torch_dtype=torch.float16,
                    device_map="auto"
                )
            else:
                # Load base Qwen Max model
                print(f"âš ï¸ æœªæ‰¾åˆ°Qji Maxå¾®è°ƒæ¨¡å‹ï¼Œä½¿ç”¨åŸºç¡€Qwen Maxæ¨¡å‹")
                print(f"ğŸ” åŸºç¡€æ¨¡å‹: {self.base_model_name}")
                self.tokenizer = AutoTokenizer.from_pretrained(self.base_model_name)
                self.model = AutoModelForCausalLM.from_pretrained(
                    self.base_model_name,
                    torch_dtype=torch.float16,
                    device_map="auto"
                )
                
                # Create models directory and save base model as Qji Max
                self.model_path.parent.mkdir(parents=True, exist_ok=True)
                print(f"ğŸ’¾ ä¿å­˜åŸºç¡€æ¨¡å‹ä¸ºQji Max: {self.model_path}")
                self.model.save_pretrained(str(self.model_path))
                self.tokenizer.save_pretrained(str(self.model_path))
            
            print("âœ… Qji Maxæ¨¡å‹åŠ è½½å®Œæˆï¼")
            return True
            
        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            print("ğŸ”„ å›é€€åˆ°æ™ºèƒ½æ¨¡æ¿ç³»ç»Ÿ...")
            return False
    
    def generate_response(self, prompt, max_length=1024, temperature=0.7):
        """Generate response using Qji Max model"""
        if self.model is None:
            return self._fallback_response(prompt)
        
        try:
            # Prepare input
            inputs = self.tokenizer(prompt, return_tensors="pt").to(self.model.device)
            
            # Generate response
            outputs = self.model.generate(
                **inputs,
                max_length=max_length,
                temperature=temperature,
                do_sample=True,
                pad_token_id=self.tokenizer.eos_token_id
            )
            
            # Decode response
            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # Remove prompt from response
            if response.startswith(prompt):
                response = response[len(prompt):].strip()
            
            return response
            
        except Exception as e:
            print(f"âŒ ç”Ÿæˆå“åº”å¤±è´¥: {e}")
            return self._fallback_response(prompt)
    
    def _fallback_response(self, prompt):
        """Fallback to intelligent template system"""
        # Import smart template engine
        from .smart_template_engine import SmartTemplateEngine
        engine = SmartTemplateEngine()
        return engine.generate_response(prompt)

# Global model manager instance
model_manager = ModelManager()